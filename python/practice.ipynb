{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from nn import *\n",
    "\n",
    "train_data = scipy.io.loadmat('../data/nist36_train.mat')\n",
    "valid_data = scipy.io.loadmat('../data/nist36_valid.mat')\n",
    "test_data = scipy.io.loadmat('../data/nist36_test.mat')\n",
    "\n",
    "train_x, train_y = train_data['train_data'], train_data['train_labels']\n",
    "valid_x, valid_y = valid_data['valid_data'], valid_data['valid_labels']\n",
    "test_x, test_y = test_data['test_data'], test_data['test_labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 00 \t loss: 33551.64 \t acc : 0.21\n"
     ]
    }
   ],
   "source": [
    "if False: # view the data\n",
    "    np.random.shuffle(train_x)\n",
    "    for crop in train_x:\n",
    "        plt.imshow(crop.reshape(32,32).T, cmap=\"Greys\")\n",
    "        plt.show()\n",
    "\n",
    "max_iters = 2\n",
    "# pick a batch size, learning rate\n",
    "batch_size = 2\n",
    "# learning_rate = 5e-2\n",
    "learning_rate = 5e-3 # Best \n",
    "hidden_size = 64\n",
    "\n",
    "## Best param, Validation accuracy:  0.773, Test accuracy:  0.7766666666666666\n",
    "# max_iters = 60\n",
    "# # pick a batch size, learning rate\n",
    "# batch_size = 2\n",
    "# learning_rate = 5e-3\n",
    "# hidden_size = 64\n",
    "##########################\n",
    "##### your code here #####\n",
    "##########################\n",
    "\n",
    "batches = get_random_batches(train_x,train_y,batch_size)\n",
    "batch_num = len(batches)\n",
    "\n",
    "params = {}\n",
    "\n",
    "# initialize layers\n",
    "initialize_weights(train_x.shape[1], hidden_size, params, \"layer1\")\n",
    "initialize_weights(hidden_size, train_y.shape[1], params, \"output\")\n",
    "layer1_W_initial = np.copy(params[\"Wlayer1\"]) # copy for Q3.3\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_acc = []\n",
    "valid_acc = []\n",
    "for itr in range(max_iters):\n",
    "    # record training and validation loss and accuracy for plotting\n",
    "    h1 = forward(train_x,params,'layer1')\n",
    "    probs = forward(h1,params,'output',softmax)\n",
    "    loss, acc = compute_loss_and_acc(train_y, probs)\n",
    "    train_loss.append(loss/train_x.shape[0])\n",
    "    train_acc.append(acc)\n",
    "    h1 = forward(valid_x,params,'layer1')\n",
    "    probs = forward(h1,params,'output',softmax)\n",
    "    loss, acc = compute_loss_and_acc(valid_y, probs)\n",
    "    valid_loss.append(loss/valid_x.shape[0])\n",
    "    valid_acc.append(acc)\n",
    "\n",
    "    total_loss = 0\n",
    "    avg_acc = 0\n",
    "    for xb,yb in batches:\n",
    "        # training loop can be exactly the same as q2!\n",
    "        ##########################\n",
    "        ##### your code here #####\n",
    "        ##########################\n",
    "        pass\n",
    "        # forward\n",
    "        h1 = forward(xb,params, 'layer1')\n",
    "        probs = forward(h1,params, 'output', softmax)\n",
    "\n",
    "        # loss\n",
    "        # be sure to add loss and accuracy to epoch totals \n",
    "        loss, acc = compute_loss_and_acc(yb, probs)\n",
    "        total_loss += loss\n",
    "        avg_acc += acc\n",
    "\n",
    "        # backward\n",
    "        delta1 = probs - yb\n",
    "        delta2 = backwards(delta1, params, 'output', linear_deriv)\n",
    "        backwards(delta2,params, 'layer1', sigmoid_deriv)\n",
    "\n",
    "        # apply gradient \n",
    "        # gradients should be summed over batch samples\n",
    "        for k,v in params.items():\n",
    "            if 'grad' in k:\n",
    "                name = k.split('_')[1]\n",
    "                param_tensor = params[name]\n",
    "                grad_tensor = v\n",
    "                param_tensor -= learning_rate * grad_tensor\n",
    "    \n",
    "    # total_loss /= len(batches)\n",
    "    avg_acc /= len(batches)\n",
    "        \n",
    "        \n",
    "    if itr % 2 == 0:\n",
    "        print(\"itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f}\".format(itr,total_loss,avg_acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfd68cfa2b9fc59dfa526be5b1fe2b11b9ac1cdfd79f98c3b49028d0371b3f9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
